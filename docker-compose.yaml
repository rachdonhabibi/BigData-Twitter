version: '2'

services:
  # ----------------------------
  # ZOOKEEPER
  # ----------------------------
  zookeeper:
    image: bitnamilegacy/zookeeper:3.9.1
    ports:
      - '2181:2181'
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    volumes:
      - zookeeper_data:/bitnami/zookeeper

  # ----------------------------
  # KAFKA
  # ----------------------------
  kafka:
    image: bitnamilegacy/kafka:3.6.1
    ports:
      - '9092:9092'
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
    depends_on:
      - zookeeper
    volumes:
      - kafka_data:/bitnami/kafka

  # ----------------------------
  # PYTHON KAFKA PRODUCER (simulation temps réel)
  # ----------------------------
  kafka-producer:
    image: python:3.11-slim
    working_dir: /app
    depends_on:
      - kafka
    volumes:
      - ./src:/app/src
      - ./data:/app/data
    command: ["bash", "-c", "pip install --no-cache-dir kafka-python pandas pyarrow && python -m src.ingestion.producer"]

  # ----------------------------
  # SPARK MASTER
  # ----------------------------
  spark-master:
    image: bitnamilegacy/spark:3.4.2
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '8080:8080'
      - '7077:7077'
    volumes:
      - ./src:/opt/bitnami/spark/src
      - ./data:/opt/bitnami/spark/data

  # ----------------------------
  # SPARK WORKER
  # ----------------------------
  spark-worker:
    image: bitnamilegacy/spark:3.4.2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    volumes:
      - ./src:/opt/bitnami/spark/src
      - ./data:/opt/bitnami/spark/data

  # ----------------------------
  # POSTGRESQL (Metastore ou Data Warehouse)
  # ----------------------------
  postgres:
    image: bitnamilegacy/postgresql:15.5.0
    ports:
      - '5432:5432'
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=reddit_db
    volumes:
      - postgres_data:/bitnami/postgresql

  # ----------------------------
  # MONGODB (Stockage NoSQL)
  # ----------------------------
  mongodb:
    image: bitnamilegacy/mongodb:7.0.4
    ports:
      - '27017:27017'
    environment:
      - MONGODB_ROOT_PASSWORD=password123
      - MONGODB_DATABASE=reddit_data
    volumes:
      - mongodb_data:/bitnami/mongodb

  # ----------------------------
  # AIRFLOW (Avec correction Host + Mode Éco)
  # ----------------------------
  airflow:
    image: bitnamilegacy/airflow:2.8.0
    ports:
      - '8081:8080'
    environment:
      - AIRFLOW_USERNAME=admin
      - AIRFLOW_PASSWORD=admin
      - AIRFLOW_EMAIL=admin@example.com
      # Utiliser le nom du service Postgres
      - AIRFLOW_DATABASE_HOST=postgres
      - AIRFLOW_DATABASE_PORT_NUMBER=5432
      # Utiliser la DB créée par Postgres
      - AIRFLOW_DATABASE_NAME=reddit_db
      - AIRFLOW_DATABASE_USERNAME=postgres      # <--- ICI (USERNAME, pas USER)
      - AIRFLOW_DATABASE_PASSWORD=postgres
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # --- MODE ÉCO ---
      - AIRFLOW__CORE__PARALLELISM=2
      - AIRFLOW__CORE__DAG_CONCURRENCY=2
      - AIRFLOW__WEBSERVER__WORKERS=1
      - AIRFLOW__CORE__DEFAULT_TASK_RETRIES=2
    depends_on:
      - postgres
    volumes:
      - ./dags:/opt/bitnami/airflow/dags
      - ./src:/opt/bitnami/airflow/src
      - ./data:/opt/bitnami/airflow/data

# ----------------------------
# VOLUMES
# ----------------------------
volumes:
  zookeeper_data:
  kafka_data:
  postgres_data:
  mongodb_data: